{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c43234f-71f6-4186-8ac5-92aa3a80bcea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "# from unidecode import unidecode  # 可选，如果不想转拼音可以不使用\n",
    "\n",
    "# Define parameters for the notebook\n",
    "dbutils.widgets.text('environment',\"\")\n",
    "\n",
    "var_environment = dbutils.widgets.get('environment')\n",
    "\n",
    "OUTPUT_PARSED = f\"/Volumes/land_market_{var_environment}/10_rawdata/transaction_info_parsed/\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in glob.glob(OUTPUT_PARSED + \"*.json\"):\n",
    "    data = json.load(open(file, encoding=\"utf-8\"))\n",
    "\n",
    "    rows = data[\"rows\"]\n",
    "    header = rows[0]          # 第一行是列名\n",
    "    body = rows[1:]           # 其他是数据\n",
    "    \n",
    "    df = pd.DataFrame(body, columns=header)\n",
    "    # 将 metadata 插入到最前面\n",
    "    df.insert(0, \"url\", data[\"url\"])\n",
    "    df.insert(0, \"publish_time\", data[\"publish_time\"])\n",
    "    df.insert(0, \"title\", data[\"title\"])\n",
    "    df.insert(0, \"transaction_id\", data[\"transaction_id\"])\n",
    "    dfs.append(df)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "spark_df = spark.createDataFrame(final_df)\n",
    "# spark_df.createOrReplaceTempView(\"v_transaction_raw\")\n",
    "# display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96907e5-1b1e-4c92-8945-9b52b9257487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_column(col: str, use_unidecode: bool = False) -> str:\n",
    "    # 1. 去两端空白，替换全角空格\n",
    "    col = col.strip().replace('\\u3000', ' ')\n",
    "    # 2. 全角转半角\n",
    "    col = ''.join(chr(ord(c) - 65248) if 65281 <= ord(c) <= 65374 else c for c in col)\n",
    "    # 3. 可选：中文转拼音/拉丁化（unidecode 会把中文变成近似拼音或字母）\n",
    "    if use_unidecode:\n",
    "        col = unidecode(col)\n",
    "\n",
    "    # 2. 单位统一替换（㎡ → sqm）\n",
    "    col = col.replace(\"㎡\", \"sqm\").replace(\"m²\", \"sqm\").replace(\"m2\", \"sqm\")\n",
    "\n",
    "    # 4. 删除括号、单位符号等常见特殊字符\n",
    "    col = re.sub(r\"[()（）\\[\\]{}<>%¥￥㎡㎡²³°′″·–—·/\\\\,:;\\\"'。、••·…\\-]\", \" \", col)\n",
    "    # 5. 删除其它非字母数字下划线空白的字符\n",
    "    col = re.sub(r\"[^\\w\\s]\", \" \", col)\n",
    "    # 6. 空白转下划线，合并多下划线\n",
    "    col = re.sub(r\"\\s+\", \"_\", col)\n",
    "    col = re.sub(r\"_+\", \"_\", col)\n",
    "    # 7. 去掉首尾下划线\n",
    "    col = col.strip(\"_\")\n",
    "    # 8. 小写\n",
    "    col = col.lower()\n",
    "    # 9. 如果为空则填一个占位\n",
    "    if not col:\n",
    "        col = \"col\"\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2280135e-8c24-4c3e-bb22-9abfe4cdda67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_unique_column_names(cols, use_unidecode=False):\n",
    "    \"\"\"\n",
    "    输入：原始列名列表\n",
    "    输出：unique_names 列表（与 cols 一一对应），以及 rename_map 原->新\n",
    "    去重策略：第一次出现保持原规范名，后续冲突追加 _1,_2...\n",
    "    也会处理以数字开头的列名（加 col_ 前缀）。\n",
    "    \"\"\"\n",
    "    normalized = [normalize_column(c, use_unidecode) for c in cols]\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    unique_names = []\n",
    "    rename_map = {}\n",
    "\n",
    "    for idx, (orig, norm) in enumerate(zip(cols, normalized)):\n",
    "        base = norm\n",
    "        # 如果以数字开头，添加前缀避免 Spark/SQL 不兼容\n",
    "        if re.match(r\"^[0-9]\", base):\n",
    "            base = \"col_\" + base\n",
    "\n",
    "        if counts[base] == 0:\n",
    "            final = base\n",
    "        else:\n",
    "            # append suffix _1, _2 ...\n",
    "            final = f\"{base}_{counts[base]}\"\n",
    "        counts[base] += 1\n",
    "\n",
    "        # 再检查 final 是否与已有任意 final 冲突（极少数情况）\n",
    "        # 如果冲突，使用索引保证唯一\n",
    "        if final in unique_names:\n",
    "            final = f\"{base}_{idx}\"\n",
    "        unique_names.append(final)\n",
    "        rename_map[orig] = final\n",
    "\n",
    "    return unique_names, rename_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fee219c-4fc6-481e-8dab-92e575bd5586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 示例：在 Spark DataFrame 上重命名\n",
    "# spark_df: 你的 DataFrame\n",
    "orig_cols = spark_df.columns\n",
    "unique_names, rename_map = make_unique_column_names(orig_cols, use_unidecode=False)  # or True\n",
    "\n",
    "# 批量重命名（顺序一一对应）\n",
    "for old, new in zip(orig_cols, unique_names):\n",
    "    if old != new:\n",
    "        spark_df = spark_df.withColumnRenamed(old, new)\n",
    "\n",
    "# 打印映射以备审计\n",
    "print(\"列名映射（原 -> 新）:\")\n",
    "for k, v in rename_map.items():\n",
    "    print(f\"{k!r} -> {v!r}\")\n",
    "\n",
    "\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"land_market_dev.20_datastore.transaction_detail\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7718737248735496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_raw_to_datastore",
   "widgets": {
    "environment": {
     "currentValue": "dev",
     "nuid": "4ead34d0-9a6f-4921-8ec0-202a041e7474",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "environment",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
